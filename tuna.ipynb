{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eceb9c81-1fae-42dd-b037-872721458224",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10925d07-d585-43c2-997a-02de77d73b70",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55029d51-76c2-498d-924a-87b3c0e52793",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Constraints\n",
    "\n",
    "We import the `constraints` module from `constraints.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716f476-6642-4c1d-952b-cb48cbb63d11",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from constraints import (Constraint, filter_df, ensure_constraint,\n",
    "                         FIELD, NOT, OR, AND, EQ, RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2b4e5-17dd-403f-901b-eacc645f633e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This module defines the abstract `Constraint` type, which exposes\n",
    "methods, `Constraint.__contains__` and `Constraint.mask`.\n",
    "\n",
    "The function `c.__contains__(x)`, that is `x in c`, returns\n",
    "`True` if `x` satisfies the constraint `c`.\n",
    "\n",
    "The function `c.mask(df)` returns a mask for the pandas dataframe or\n",
    "series `df` which, when applied (as `df[c.mask(df)]`), returns the\n",
    "rows of `df` which satisfy `c`.\n",
    "\n",
    "Because `df[c.mask(df)]` is such a common thing to do, the function \n",
    "`filter_df` is defined as a shorthand:\n",
    "\n",
    "``` python\n",
    "def filter_df(df, constraint):\n",
    "  return df[ensure_constraint(constraint).mask(df)]\n",
    "```\n",
    "\n",
    "The function `ensure_constraint` is the way to convert\n",
    "non-`Constraint` objects into `Constraint` objects, such that our\n",
    "methods are well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51acea-0268-407f-baa7-45e9d8a23371",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Org Babel Setup\n",
    "\n",
    "``` python\n",
    "def assStr (name, literal=True):\n",
    "  f = repr if literal else str\n",
    "  return name + \" = \" + f(eval(name))\n",
    "```\n",
    "\n",
    "``` elisp\n",
    "(message \"%s\" v)\n",
    "```\n",
    "\n",
    "``` elisp\n",
    "(when (eq 'hline (cadr table))\n",
    "  (setq table (cddr table)))\n",
    "(cl-flet ((ellide-list (list length &optional (ellision \"...\"))\n",
    "              (if (and length (length> list length))\n",
    "                  (append (seq-subseq list 0 (/ length 2))\n",
    "                          (list ellision)\n",
    "                          (seq-subseq list (- (length list) (/ length 2))))\n",
    "                list)))\n",
    "    (mapcar (lambda (row) (if (listp row) (ellide-list row ncols) row))\n",
    "            (ellide-list table nrows (make-list (length (car table)) \"...\"))))\n",
    "```\n",
    "\n",
    "``` elisp\n",
    "(cl-mapcar #'list (number-sequence start (+ start (length list))) list)\n",
    "```\n",
    "\n",
    "``` elisp\n",
    "(cl-loop with i = (1- start)\n",
    "         for startp = t then nil\n",
    "         for tail on table\n",
    "         collect\n",
    "         (cond ((atom (car tail)) (car tail))\n",
    "               ((and startp (eq 'hline (cadr tail)))\n",
    "                (cons index-name (car tail)))\n",
    "               (t (cons (cl-incf i) (car tail)))))\n",
    "```\n",
    "\n",
    "# Setup\n",
    "\n",
    "``` python\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "# Download Sessions\n",
    "\n",
    "Set `data_dir` and ensure it exists.\n",
    "\n",
    "``` python\n",
    "from pathlib import Path\n",
    "data_dir: Final[Path] = Path('./data/')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "```\n",
    "\n",
    "Set `manifest_path`.\n",
    "\n",
    "``` python\n",
    "manifest_path: Final[Path] = data_dir / 'manifest.json'\n",
    "```\n",
    "\n",
    "Create and initialize the project cache object\n",
    "\n",
    "``` python\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "cache: Final = EcephysProjectCache.from_warehouse(manifest=manifest_path, timeout=30*60)\n",
    "```\n",
    "\n",
    "and obtain the sessions\n",
    "\n",
    "``` python\n",
    "sessions = cache.get_session_table()\n",
    "```\n",
    "\n",
    "Extract the session ids from `sessions` into `session_ids`\n",
    "\n",
    "``` python\n",
    "session_ids: Final[Sequence[int]] = list(sessions.index)\n",
    "```\n",
    "\n",
    "Extract a single session into `session`:\n",
    "\n",
    "``` python\n",
    "assert session_id in session_ids\n",
    "session = cache.get_session_data(session_id) \n",
    "session.metadata\n",
    "```\n",
    "\n",
    "``` python\n",
    "session.units.head()\n",
    "```\n",
    "\n",
    "``` python\n",
    "session.stimulus_conditions.head()\n",
    "```\n",
    "\n",
    "``` python\n",
    "session.stimulus_conditions['stimulus_name'].value_counts()\n",
    "```\n",
    "\n",
    "``` python\n",
    "grouped_stimulus_presentations = session.stimulus_presentations.groupby('stimulus_name')\n",
    "```\n",
    "\n",
    "``` python\n",
    "total_durations = grouped_stimulus_presentations['duration'].sum()\n",
    "```\n",
    "\n",
    "``` python\n",
    "def presentation_type_spike_times(expr = None, mask = None, unit_names = session.units.index):\n",
    "  presentations = session.stimulus_presentations\n",
    "  if mask is not None:\n",
    "    presentations = presentations[mask]\n",
    "  if expr is not None:\n",
    "    presentations = presentations.query(expr)\n",
    "  return session.presentationwise_spike_times(presentations['stimulus_condition_id'], unit_names)\n",
    "\n",
    "presentation_type_spike_times(expr=\"`stimulus_name` == 'drifting_gratings'\").head(10)\n",
    "```\n",
    "\n",
    "``` python\n",
    "spike_times_with_presentations_condition_names = \\\n",
    "  pd.merge(\n",
    "    session.presentationwise_spike_times(\n",
    "      session.stimulus_presentations['stimulus_condition_id'],\n",
    "      session.units.index).reset_index(),\n",
    "    session.stimulus_presentations['stimulus_name'],\n",
    "    how='left',\n",
    "    on='stimulus_presentation_id'\n",
    "  )\n",
    "```\n",
    "\n",
    "``` python\n",
    "def get_spike_times_df(unit_ids: Optional[Sequence[int]]=None):\n",
    "  return pd.concat((\n",
    "    (_df := pd.DataFrame(unit_spike_times, columns=['spike_time']),\n",
    "     _df.insert(0,'unit_id',unit_id),\n",
    "     _df)[-1]\n",
    "    for (unit_id, unit_spike_times) in session.spike_times.items()\n",
    "    if unit_ids is None or unit_id in unit_ids\n",
    "  ), ignore_index=True, copy=False)\n",
    "\n",
    "spike_times_df = get_spike_times_df()\n",
    "```\n",
    "\n",
    "``` python\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "name": "tuna.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
